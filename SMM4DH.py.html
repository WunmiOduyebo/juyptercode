#!/usr/bin/env python
# coding: utf-8

# In[24]:


import re
import csv
from ekphrasis.classes.preprocessor import TextPreProcessor
#from ekphrasis.classes.tokenizer import SocialTokenizer
from ekphrasis.dicts.emoticons import emoticons
from ekphrasis.classes.segmenter import Segmenter
from nltk.tokenize import TweetTokenizer


# In[25]:


id=[]
data = []
target = []
for line in open('/Users/si-guest/Desktop/task1_training_no_utf.tsv'):
    line_sep = line.split('\t')

    
    id.append(line_sep[0])
    data.append(line_sep[3])
    target.append(line_sep[2])


# In[26]:


line_sep


# In[27]:


id


# In[28]:


text_processor = TextPreProcessor(
    # terms that will be normalized
    normalize=['url', 'email', 'percent', 'money', 'phone',
        'time', 'date'],
    # terms that will be annotated
    annotate={"hashtag", "allcaps", "elongated", "repeated",
        'emphasis', 'censored'},
    fix_html=True,  # fix HTML tokens

    # corpus from which the word statistics are going to be used
    # for word segmentation
    segmenter="twitter",

    # corpus from which the word statistics are going to be used
    # for spell correction
    corrector="twitter",

    unpack_hashtags=True,  # perform word segmentation on hashtags
    # unpack_users=True, # dunno if this is a thing
    unpack_contractions=True,  # Unpack contractions (can't -> can not)
    spell_correct_elong=False,  # spell correction for elongated words

    # select a tokenizer. You can use SocialTokenizer, or pass your own
    # the tokenizer, should take as input a string and return a list of tokens
    #tokenizer=SocialTokenizer(lowercase=True).tokenize,
    tokenizer=TweetTokenizer().tokenize,

    # list of dictionaries, for replacing tokens extracted from the text,
    # with other expressions. You can pass more than one dictionaries.
    dicts=[emoticons]
)
seg = Segmenter(corpus="twitter")


# In[29]:


clean_tweets = []
for tweet in data:

    # manually tag usernames
    # ex: @DoctorChristian -> <user> doctor christian </user>
    match = re.findall(r'@\w+', tweet)

    try:
       for at in match:
           user_seg = seg.segment(at[1:])
           tweet = tweet.replace(at, '<user> ' + user_seg + ' </user>')
    except:
        None

    # deal with contractions that the tool misses
    tweet = re.sub(r"(\b)([Ww]hat|[Ii]t|[Hh]e|[Ss]he|[Tt]hat|[Tt]here|[Hh]ow|[Ww]ho|[Hh]ere|[Ww]here|[Ww]hen)'s", r"\1\2 is", tweet)
    tweet = re.sub(r"(\b)([Aa]in)'t", r"is not", tweet)
    tweet = re.sub(r"(\b)([Ww]asn)'t", r"was not", tweet)
    tweet = re.sub(r"(\b)([Hh]e|[Ss]he|[Ii]|[Yy]ou|[Tt]hey|[Ww]e)'d", r"\1\2 would", tweet)
    tweet = re.sub(r"(\b)([Ii]t|[Tt]hat|[Tt]his)'ll", r"\1\2 will", tweet)
    tweet = re.sub(r"(\b)([Cc])'mon", r"come on", tweet)

    # process the rest of the tweet with the nltk tweet tokenizer
    tweet = " ".join(text_processor.pre_process_doc(tweet)).lower()

    clean_tweets.append(tweet)


# In[30]:


clean_tweets


# In[31]:


index = 0
with open('task1_training_cleaned.tsv', mode='w') as tsvfile:
    tsvwriter = csv.writer(tsvfile, delimiter='\t')

    index = 0
    for tweet in clean_tweets:
        tsvwriter.writerow([tweet, id[index], target[index]])

        index += 1
tsvfile.close()


# In[ ]:


# GloVe Embeddings- I utilized the Twitter GloVe from https://www.kaggle.com/jdpaletto/glove-global-vectors-for-word-representation#glove.twitter.27B.200d.txt
import numpy as np
def loadGloveModel(gloveFile):
    print("Loading Glove Model")
    f = open(gloveFile,'r')
    model = {}
    for line in f:
        splitLine = line.split()
        word = splitLine[0]
        embedding = np.array([float(val) for val in splitLine[1:]])
        model[word] = embedding
    print("Done.",len(model)," words loaded!")
    print("new", model)
    return model

y= loadGloveModel("/Users/si-guest/Desktop/glove.twitter.27B.100d.txt")


# In[ ]:


import pandas as pd
import csv
glove_data_file= "/Users/si-guest/Desktop/glove.twitter.27B.100d.txt"

words = pd.read_table(glove_data_file, sep=" ", index_col=0, header=None, quoting=csv.QUOTE_NONE)

def vec(w):
    print(words.loc[w].as_matrix())
    return words.loc[w].as_matrix()
    


# In[ ]:


words_matrix = words.as_matrix()

def find_closest_word(v):
    diff = words_matrix - v
    delta = np.sum(diff * diff, axis=1)
    i = np.argmin(delta)
    print(i)
    return words.iloc[i].name


# In[ ]:




